{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999d0a14",
   "metadata": {},
   "source": [
    "Name: Nishit Shaileshbhai Rathod     \n",
    "Student No: N01586439"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01cbc4e",
   "metadata": {},
   "source": [
    "# Tabular regression with Amazon SageMaker LightGBM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5bfd9",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of Amazon SageMaker's implementation of LightGBM algorithm to train and host a tabular regression model. Tabular regression is the task of analyzing the relationship between predictor variables and a response variable in a structured or relational data.\n",
    "\n",
    "In this notebook, we demonstrate two use cases of tabular regression models:\n",
    "\n",
    "- How to train a tabular model on an example dataset to do regression.\n",
    "- How to use the trained tabular model to perform inference, i.e., predicting new samples.\n",
    "\n",
    "Note: This notebook was tested in Amazon SageMaker Studio on ml.t3.medium instance with Python 3 (Data Science) kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93152d",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Before executing the notebook, there are some initial steps required for setup.\n",
    "\n",
    "To train and host on Amazon SageMaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe59bf8-b180-4cf9-a638-402ff20fdd28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92ac37",
   "metadata": {},
   "source": [
    "# Train a Tabular Model on Abalone Dataset\n",
    "\n",
    "In this demonstration, we will train a tabular algorithm on the University dataset. The dataset contains examples of 7 features and a target value.\n",
    "\n",
    "If you want to bring your own dataset, below are the instructions on how the training data should be formatted as input to the model.\n",
    "\n",
    "A S3 path should contain two sub-directories 'train/', 'validation/' (optional), and a json-format file named 'categorical_index.json' (optional). Each sub-directory contains a 'data.csv' file (The Abalone dataset used in this example has been prepared and saved in training_dataset_s3_path shown below).\n",
    "\n",
    "- The 'data.csv' files under sub-directory 'train/' and 'validation/' are for training and validation, respectively. The validation data is used to compute a validation score at the end of each boosting iteration. An early stopping is applied when the validation score stops improving. If the validation data is not provided, a 20% of training data is randomly sampled to serve as the validation data.\n",
    "\n",
    "- The first column of the 'data.csv' should have the corresponding target variable. The rest of other columns should have the corresponding predictor variables (features).\n",
    "\n",
    "- If the predictors include categorical feature(s), a json-format file named 'categorical_index.json' should be included in the input directory to indicate the column index(es) of the categorical features. Within the json-format file, it should have a python directory where the key is a string of 'cat_index_list' and the value is a list of unique integer(s). Each integer in the list indicates the column index of categorical features in the 'data.csv'. The range of each integer should be more than 0 (index 0 indicates the target) and less than the total number of columns.\n",
    "\n",
    "- All the categorical features and the target must be encoded as non-negative integers (int) less than Int32.MaxValue (2147483647). It is best to use a contiguous range of integers started from zero.\n",
    "\n",
    "- Note. The number of json-format files should be no more than 1 in the input directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc991a4",
   "metadata": {},
   "source": [
    "# Retrieve Training Artifacts\n",
    "\n",
    "Here, we retrieve the training docker container, the training algorithm source, and the tabular algorithm. Note that model_version=\"*\" fetches the latest model.\n",
    "\n",
    "For the training algorithm, we have two choices in this demonstration.\n",
    "\n",
    "- LightGBM: To use this algorithm, specify train_model_id as lightgbm-regression-model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be2a031-8864-4347-b8cb-11afdd33aef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-regression-model\", \"*\", \"training\"\n",
    "\n",
    "training_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8528d99",
   "metadata": {},
   "source": [
    "# Set Training Parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to train our tabular algorithm. To begin, let us create a sageMaker.estimator.Estimator object. This estimator will launch the training job.\n",
    "\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training.\n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336ffed7-680b-4417-9cef-c4486af31f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_data_bucket = f\"nr-jumpstart-9\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://nr-jumpstart-9/train/data.csv\" \n",
    "validation_dataset_s3_path = f\"s3://nr-jumpstart-9/validation/data.csv\" \n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-university-tab-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329f7fa",
   "metadata": {},
   "source": [
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556a021f-6c10-4997-8899-b9c3c046375f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '5000', 'early_stopping_rounds': '30', 'metric': 'auto', 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'tree_learner': 'serial', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'tweedie_variance_power': '1.5', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6f6f9",
   "metadata": {},
   "source": [
    "# Start Training\n",
    "We start by creating the estimator object with all the required assets and then launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55669cb-6a56-41c0-8efc-7222b9f4c64f",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-lightgbm-regression-model-tra-2023-04-03-22-26-03-490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-03 22:26:04 Starting - Starting the training job...\n",
      "2023-04-03 22:26:18 Starting - Preparing the instances for training...\n",
      "2023-04-03 22:26:57 Downloading - Downloading input data...\n",
      "2023-04-03 22:27:43 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:49,120 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:49,122 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:49,130 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:49,132 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:49,306 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->dask==2022.12.1->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->distributed==2022.12.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: toolz, locket, partd, HeapDict, zict, tblib, sortedcontainers, msgpack, dask, sagemaker-jumpstart-tabular-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[34mSuccessfully installed HeapDict-1.0.1 dask-2022.12.1 distributed-2022.12.1 graphviz-0.17 lightgbm-3.3.3 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:52,082 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:52,094 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:52,104 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:52,112 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"early_stopping_rounds\": \"30\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auto\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"5000\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"tree_learner\": \"serial\",\n",
      "        \"tweedie_variance_power\": \"1.5\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"jumpstart-lightgbm-regression-model-tra-2023-04-03-22-26-03-490\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-ca-central-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"5000\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"tree_learner\":\"serial\",\"tweedie_variance_power\":\"1.5\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-ca-central-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"5000\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"tree_learner\":\"serial\",\"tweedie_variance_power\":\"1.5\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"jumpstart-lightgbm-regression-model-tra-2023-04-03-22-26-03-490\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-ca-central-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--early_stopping_rounds\",\"30\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auto\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"5000\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--tree_learner\",\"serial\",\"--tweedie_variance_power\",\"1.5\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=30\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC=auto\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=5000\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_LEARNER=serial\u001b[0m\n",
      "\u001b[34mSM_HP_TWEEDIE_VARIANCE_POWER=1.5\u001b[0m\n",
      "\u001b[34mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --early_stopping_rounds 30 --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auto --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 5000 --num_leaves 67 --num_threads 0 --tree_learner serial --tweedie_variance_power 1.5 --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading data\u001b[0m\n",
      "\u001b[34mINFO:root:'ContentType' is not identified in either training or validation data channel. Default ContentType 'text/csv' is used to read the train and validation data.\u001b[0m\n",
      "\u001b[34mINFO:root:Found data in the validation channel. Reading the train and validation data from the training and validation channel, respectively.\u001b[0m\n",
      "\u001b[34mdata frame ['/opt/ml/input/data/train/data.csv']???         0      1      2    3    4    5     6    7\u001b[0m\n",
      "\u001b[34m0    0.66  310.0  106.0  4.0  4.5  4.5  9.04  1.0\u001b[0m\n",
      "\u001b[34m1    0.86  330.0  114.0  4.0  4.5  3.0  9.17  1.0\u001b[0m\n",
      "\u001b[34m2    0.72  316.0  106.0  2.0  2.5  4.0  8.32  0.0\u001b[0m\n",
      "\u001b[34m3    0.50  302.0  102.0  1.0  2.0  1.5  8.00  0.0\u001b[0m\n",
      "\u001b[34m4    0.64  303.0  100.0  2.0  3.0  3.5  8.06  1.0\u001b[0m\n",
      "\u001b[34m..    ...    ...    ...  ...  ...  ...   ...  ...\u001b[0m\n",
      "\u001b[34m695  0.52  325.0  111.0  3.0  3.0  3.5  8.70  0.0\u001b[0m\n",
      "\u001b[34m696  0.66  308.0  103.0  2.0  3.0  3.5  8.49  0.0\u001b[0m\n",
      "\u001b[34m697  0.77  313.0  102.0  3.0  3.5  4.0  8.90  1.0\u001b[0m\n",
      "\u001b[34m698  0.72  324.0  112.0  4.0  4.0  2.5  8.10  1.0\u001b[0m\n",
      "\u001b[34m699  0.42  311.0  104.0  2.0  2.0  2.0  8.30  0.0\u001b[0m\n",
      "\u001b[34m[700 rows x 8 columns]\u001b[0m\n",
      "\u001b[34mdata frame ['/opt/ml/input/data/validation/data.csv']???         0      1      2    3    4    5     6    7\u001b[0m\n",
      "\u001b[34m0    0.64  312.0  110.0  2.0  3.5  3.0  8.53  0.0\u001b[0m\n",
      "\u001b[34m1    0.78  307.0  107.0  2.0  3.0  3.5  8.52  1.0\u001b[0m\n",
      "\u001b[34m2    0.81  324.0  107.0  5.0  3.5  4.0  8.66  1.0\u001b[0m\n",
      "\u001b[34m3    0.51  299.0  100.0  2.0  2.0  2.0  7.88  0.0\u001b[0m\n",
      "\u001b[34m4    0.87  327.0  108.0  5.0  5.0  3.5  9.13  1.0\u001b[0m\n",
      "\u001b[34m..    ...    ...    ...  ...  ...  ...   ...  ...\u001b[0m\n",
      "\u001b[34m145  0.57  315.0  103.0  1.0  1.5  2.0  7.86  0.0\u001b[0m\n",
      "\u001b[34m146  0.57  317.0  104.0  2.0  4.5  4.0  8.47  0.0\u001b[0m\n",
      "\u001b[34m147  0.79  313.0  109.0  3.0  4.0  3.5  9.00  0.0\u001b[0m\n",
      "\u001b[34m148  0.73  319.0  103.0  3.0  2.5  4.0  8.76  1.0\u001b[0m\n",
      "\u001b[34m149  0.52  325.0  111.0  3.0  3.0  3.5  8.70  0.0\u001b[0m\n",
      "\u001b[34m[150 rows x 8 columns]\u001b[0m\n",
      "\u001b[34mINFO:root:'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mINFO:root:Beginning training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Total Bins 235\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 7\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Start training from score 0.725000\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[1]#011train's rmse: 0.140823#011val's rmse: 0.1332\u001b[0m\n",
      "\u001b[34mTraining until validation scores don't improve for 30 rounds\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[2]#011train's rmse: 0.139824#011val's rmse: 0.13221\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[3]#011train's rmse: 0.138834#011val's rmse: 0.131224\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[4]#011train's rmse: 0.137855#011val's rmse: 0.130249\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[5]#011train's rmse: 0.136958#011val's rmse: 0.129374\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[6]#011train's rmse: 0.136095#011val's rmse: 0.128546\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[7]#011train's rmse: 0.135147#011val's rmse: 0.127596\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[8]#011train's rmse: 0.134288#011val's rmse: 0.126746\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[9]#011train's rmse: 0.133334#011val's rmse: 0.125786\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[10]#011train's rmse: 0.13239#011val's rmse: 0.124835\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[11]#011train's rmse: 0.131482#011val's rmse: 0.123957\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[12]#011train's rmse: 0.130584#011val's rmse: 0.12309\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[13]#011train's rmse: 0.129697#011val's rmse: 0.122232\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[14]#011train's rmse: 0.128815#011val's rmse: 0.121395\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[15]#011train's rmse: 0.127946#011val's rmse: 0.120544\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[16]#011train's rmse: 0.127039#011val's rmse: 0.119674\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[17]#011train's rmse: 0.126185#011val's rmse: 0.118841\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[18]#011train's rmse: 0.125299#011val's rmse: 0.117988\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[19]#011train's rmse: 0.124422#011val's rmse: 0.117153\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[20]#011train's rmse: 0.123551#011val's rmse: 0.116312\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[21]#011train's rmse: 0.122713#011val's rmse: 0.115492\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[22]#011train's rmse: 0.121887#011val's rmse: 0.114674\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[23]#011train's rmse: 0.121067#011val's rmse: 0.113873\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[24]#011train's rmse: 0.120262#011val's rmse: 0.113086\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[25]#011train's rmse: 0.119461#011val's rmse: 0.11229\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[26]#011train's rmse: 0.118675#011val's rmse: 0.111496\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[27]#011train's rmse: 0.117906#011val's rmse: 0.110734\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[28]#011train's rmse: 0.117173#011val's rmse: 0.110003\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[29]#011train's rmse: 0.116451#011val's rmse: 0.109283\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[30]#011train's rmse: 0.115698#011val's rmse: 0.108533\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[31]#011train's rmse: 0.114944#011val's rmse: 0.107777\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[32]#011train's rmse: 0.114194#011val's rmse: 0.107033\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[33]#011train's rmse: 0.113452#011val's rmse: 0.106296\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[34]#011train's rmse: 0.112732#011val's rmse: 0.105584\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[35]#011train's rmse: 0.112001#011val's rmse: 0.104866\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[36]#011train's rmse: 0.111295#011val's rmse: 0.104161\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[37]#011train's rmse: 0.110641#011val's rmse: 0.103514\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[38]#011train's rmse: 0.10995#011val's rmse: 0.102812\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[39]#011train's rmse: 0.109311#011val's rmse: 0.10218\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[40]#011train's rmse: 0.108634#011val's rmse: 0.101506\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[41]#011train's rmse: 0.108015#011val's rmse: 0.100921\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[42]#011train's rmse: 0.107353#011val's rmse: 0.10026\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[43]#011train's rmse: 0.10675#011val's rmse: 0.0996956\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[44]#011train's rmse: 0.106102#011val's rmse: 0.0990497\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[45]#011train's rmse: 0.10546#011val's rmse: 0.0984276\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[46]#011train's rmse: 0.104867#011val's rmse: 0.0978721\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[47]#011train's rmse: 0.104254#011val's rmse: 0.097276\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[48]#011train's rmse: 0.103649#011val's rmse: 0.0966889\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[49]#011train's rmse: 0.103049#011val's rmse: 0.0960968\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[50]#011train's rmse: 0.102503#011val's rmse: 0.0956127\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[51]#011train's rmse: 0.101975#011val's rmse: 0.095084\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[52]#011train's rmse: 0.10141#011val's rmse: 0.0945409\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[53]#011train's rmse: 0.100856#011val's rmse: 0.094012\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[54]#011train's rmse: 0.10034#011val's rmse: 0.0934947\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[55]#011train's rmse: 0.0998093#011val's rmse: 0.0929711\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[56]#011train's rmse: 0.0992435#011val's rmse: 0.0924419\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[57]#011train's rmse: 0.0987408#011val's rmse: 0.0919688\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[58]#011train's rmse: 0.0982406#011val's rmse: 0.091501\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[59]#011train's rmse: 0.0977469#011val's rmse: 0.0910401\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[60]#011train's rmse: 0.09722#011val's rmse: 0.0905205\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[61]#011train's rmse: 0.0966921#011val's rmse: 0.08999\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[62]#011train's rmse: 0.0961736#011val's rmse: 0.0894743\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[63]#011train's rmse: 0.095689#011val's rmse: 0.0890146\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[64]#011train's rmse: 0.0951809#011val's rmse: 0.0885096\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[65]#011train's rmse: 0.0946808#011val's rmse: 0.0880061\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[66]#011train's rmse: 0.0942111#011val's rmse: 0.0875515\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[67]#011train's rmse: 0.0937343#011val's rmse: 0.0871072\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[68]#011train's rmse: 0.0932373#011val's rmse: 0.08663\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[69]#011train's rmse: 0.0927519#011val's rmse: 0.0861497\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[70]#011train's rmse: 0.0922966#011val's rmse: 0.0857214\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[71]#011train's rmse: 0.0918137#011val's rmse: 0.0852576\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[72]#011train's rmse: 0.0913338#011val's rmse: 0.0847952\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[73]#011train's rmse: 0.0908746#011val's rmse: 0.0843861\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[74]#011train's rmse: 0.0903999#011val's rmse: 0.0839294\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[75]#011train's rmse: 0.0899683#011val's rmse: 0.0835515\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[76]#011train's rmse: 0.0895289#011val's rmse: 0.0831468\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[77]#011train's rmse: 0.0890714#011val's rmse: 0.0827328\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[78]#011train's rmse: 0.0886352#011val's rmse: 0.0823297\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[79]#011train's rmse: 0.0881895#011val's rmse: 0.0819353\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[80]#011train's rmse: 0.0877498#011val's rmse: 0.0815467\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[81]#011train's rmse: 0.0873047#011val's rmse: 0.0811256\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[82]#011train's rmse: 0.0868631#011val's rmse: 0.0807081\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[83]#011train's rmse: 0.0864298#011val's rmse: 0.0802994\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[84]#011train's rmse: 0.0860551#011val's rmse: 0.0799651\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[85]#011train's rmse: 0.085657#011val's rmse: 0.0796093\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[86]#011train's rmse: 0.0852462#011val's rmse: 0.0792264\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[87]#011train's rmse: 0.0848436#011val's rmse: 0.0788414\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[88]#011train's rmse: 0.084478#011val's rmse: 0.0784768\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[89]#011train's rmse: 0.0841059#011val's rmse: 0.0781341\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[90]#011train's rmse: 0.083723#011val's rmse: 0.0777705\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[91]#011train's rmse: 0.0833478#011val's rmse: 0.0774243\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[92]#011train's rmse: 0.0829915#011val's rmse: 0.0771\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[93]#011train's rmse: 0.0826123#011val's rmse: 0.0767329\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[94]#011train's rmse: 0.082249#011val's rmse: 0.0763929\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[95]#011train's rmse: 0.0818722#011val's rmse: 0.0760293\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[96]#011train's rmse: 0.0815394#011val's rmse: 0.0757229\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[97]#011train's rmse: 0.0811861#011val's rmse: 0.075403\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[98]#011train's rmse: 0.0808654#011val's rmse: 0.0751194\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[99]#011train's rmse: 0.080546#011val's rmse: 0.0748263\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[100]#011train's rmse: 0.0802105#011val's rmse: 0.0745205\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[101]#011train's rmse: 0.0798823#011val's rmse: 0.0742266\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[102]#011train's rmse: 0.0795589#011val's rmse: 0.0739377\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[103]#011train's rmse: 0.0792443#011val's rmse: 0.0736433\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[104]#011train's rmse: 0.0789224#011val's rmse: 0.0733588\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[105]#011train's rmse: 0.0786121#011val's rmse: 0.0730834\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[106]#011train's rmse: 0.078277#011val's rmse: 0.0727717\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[107]#011train's rmse: 0.0779671#011val's rmse: 0.0725154\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[108]#011train's rmse: 0.0776416#011val's rmse: 0.0722285\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[109]#011train's rmse: 0.077312#011val's rmse: 0.071934\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[110]#011train's rmse: 0.0769842#011val's rmse: 0.0716391\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[111]#011train's rmse: 0.0766985#011val's rmse: 0.0713611\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[112]#011train's rmse: 0.0764546#011val's rmse: 0.0711523\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[113]#011train's rmse: 0.0761717#011val's rmse: 0.0708829\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[114]#011train's rmse: 0.0758874#011val's rmse: 0.0706148\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[115]#011train's rmse: 0.0756352#011val's rmse: 0.070376\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[116]#011train's rmse: 0.0753689#011val's rmse: 0.0701493\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[117]#011train's rmse: 0.0750893#011val's rmse: 0.0698976\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[118]#011train's rmse: 0.0748216#011val's rmse: 0.0696723\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[119]#011train's rmse: 0.074554#011val's rmse: 0.0694332\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[120]#011train's rmse: 0.0742904#011val's rmse: 0.0691983\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[121]#011train's rmse: 0.0740422#011val's rmse: 0.0689812\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[122]#011train's rmse: 0.0737836#011val's rmse: 0.0687493\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[123]#011train's rmse: 0.0735249#011val's rmse: 0.0685199\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[124]#011train's rmse: 0.0732889#011val's rmse: 0.0682982\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[125]#011train's rmse: 0.07306#011val's rmse: 0.0680895\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[126]#011train's rmse: 0.0727899#011val's rmse: 0.0678342\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[127]#011train's rmse: 0.072524#011val's rmse: 0.067583\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[128]#011train's rmse: 0.0722665#011val's rmse: 0.0673719\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[129]#011train's rmse: 0.0720075#011val's rmse: 0.0671313\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[130]#011train's rmse: 0.0717526#011val's rmse: 0.0668948\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[131]#011train's rmse: 0.0715282#011val's rmse: 0.0667043\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[132]#011train's rmse: 0.0713011#011val's rmse: 0.066493\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[133]#011train's rmse: 0.0710669#011val's rmse: 0.0662705\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[134]#011train's rmse: 0.0708468#011val's rmse: 0.0660662\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[135]#011train's rmse: 0.0706233#011val's rmse: 0.0658629\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[136]#011train's rmse: 0.0704159#011val's rmse: 0.065676\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[137]#011train's rmse: 0.0702119#011val's rmse: 0.0655012\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[138]#011train's rmse: 0.0700123#011val's rmse: 0.0653237\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[139]#011train's rmse: 0.0698284#011val's rmse: 0.0651892\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[140]#011train's rmse: 0.0696303#011val's rmse: 0.0650075\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[141]#011train's rmse: 0.0694182#011val's rmse: 0.064815\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[142]#011train's rmse: 0.0692071#011val's rmse: 0.0646234\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[143]#011train's rmse: 0.0689997#011val's rmse: 0.0644355\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[144]#011train's rmse: 0.068799#011val's rmse: 0.0642594\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[145]#011train's rmse: 0.0685997#011val's rmse: 0.0640878\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[146]#011train's rmse: 0.0684254#011val's rmse: 0.0639523\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[147]#011train's rmse: 0.0682366#011val's rmse: 0.0637831\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[148]#011train's rmse: 0.0680389#011val's rmse: 0.0636166\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[149]#011train's rmse: 0.0678496#011val's rmse: 0.0634381\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[150]#011train's rmse: 0.0676626#011val's rmse: 0.0632899\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[151]#011train's rmse: 0.0674995#011val's rmse: 0.0631455\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[152]#011train's rmse: 0.0673429#011val's rmse: 0.0630287\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[153]#011train's rmse: 0.0671813#011val's rmse: 0.0628742\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[154]#011train's rmse: 0.0670254#011val's rmse: 0.0627369\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[155]#011train's rmse: 0.06689#011val's rmse: 0.0626161\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[156]#011train's rmse: 0.0667334#011val's rmse: 0.0624692\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[157]#011train's rmse: 0.0665779#011val's rmse: 0.0623146\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[158]#011train's rmse: 0.0664256#011val's rmse: 0.062169\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[159]#011train's rmse: 0.0662829#011val's rmse: 0.0620339\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[160]#011train's rmse: 0.0661312#011val's rmse: 0.0619016\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[161]#011train's rmse: 0.0659909#011val's rmse: 0.0617934\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[162]#011train's rmse: 0.0658514#011val's rmse: 0.0616946\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[163]#011train's rmse: 0.0656972#011val's rmse: 0.061576\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[164]#011train's rmse: 0.0655422#011val's rmse: 0.0614582\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[165]#011train's rmse: 0.0653934#011val's rmse: 0.0613446\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[166]#011train's rmse: 0.0652346#011val's rmse: 0.0612059\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[167]#011train's rmse: 0.0650725#011val's rmse: 0.0610623\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[168]#011train's rmse: 0.0649145#011val's rmse: 0.0609298\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[169]#011train's rmse: 0.0647775#011val's rmse: 0.0608401\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[170]#011train's rmse: 0.0646503#011val's rmse: 0.0607713\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[171]#011train's rmse: 0.064511#011val's rmse: 0.0606598\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[172]#011train's rmse: 0.0643756#011val's rmse: 0.0605444\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[173]#011train's rmse: 0.0642406#011val's rmse: 0.0604372\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[174]#011train's rmse: 0.0641087#011val's rmse: 0.0603285\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[175]#011train's rmse: 0.0639795#011val's rmse: 0.0602189\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[176]#011train's rmse: 0.0638351#011val's rmse: 0.0601096\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[177]#011train's rmse: 0.0637303#011val's rmse: 0.0600155\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[178]#011train's rmse: 0.0635914#011val's rmse: 0.059897\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[179]#011train's rmse: 0.0634693#011val's rmse: 0.0598154\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[180]#011train's rmse: 0.0633506#011val's rmse: 0.0597314\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[181]#011train's rmse: 0.0632129#011val's rmse: 0.0596078\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[182]#011train's rmse: 0.0630814#011val's rmse: 0.0595058\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[183]#011train's rmse: 0.0629415#011val's rmse: 0.0593726\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[184]#011train's rmse: 0.0628073#011val's rmse: 0.059253\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[185]#011train's rmse: 0.0626908#011val's rmse: 0.0591505\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[186]#011train's rmse: 0.0625698#011val's rmse: 0.0590446\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[187]#011train's rmse: 0.062443#011val's rmse: 0.0589288\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[188]#011train's rmse: 0.0623234#011val's rmse: 0.0588216\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[189]#011train's rmse: 0.062206#011val's rmse: 0.0587164\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[190]#011train's rmse: 0.0620908#011val's rmse: 0.0586133\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[191]#011train's rmse: 0.0620014#011val's rmse: 0.0585479\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[192]#011train's rmse: 0.0619049#011val's rmse: 0.0584803\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[193]#011train's rmse: 0.0618046#011val's rmse: 0.0584121\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[194]#011train's rmse: 0.0617073#011val's rmse: 0.0583402\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[195]#011train's rmse: 0.061613#011val's rmse: 0.0582632\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[196]#011train's rmse: 0.061516#011val's rmse: 0.0581906\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[197]#011train's rmse: 0.061428#011val's rmse: 0.0581133\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[198]#011train's rmse: 0.0613269#011val's rmse: 0.0580148\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[199]#011train's rmse: 0.0612266#011val's rmse: 0.0579415\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[200]#011train's rmse: 0.0611301#011val's rmse: 0.0578843\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[201]#011train's rmse: 0.0610305#011val's rmse: 0.0578019\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[202]#011train's rmse: 0.0609416#011val's rmse: 0.0577166\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[203]#011train's rmse: 0.0608619#011val's rmse: 0.0576604\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[204]#011train's rmse: 0.0607743#011val's rmse: 0.0575824\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[205]#011train's rmse: 0.0606914#011val's rmse: 0.0575017\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[206]#011train's rmse: 0.0606239#011val's rmse: 0.05745\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[207]#011train's rmse: 0.0605574#011val's rmse: 0.0573891\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[208]#011train's rmse: 0.0604975#011val's rmse: 0.0573401\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[209]#011train's rmse: 0.0604238#011val's rmse: 0.0572979\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[210]#011train's rmse: 0.0603684#011val's rmse: 0.0572624\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[211]#011train's rmse: 0.0602803#011val's rmse: 0.0571837\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[212]#011train's rmse: 0.0602038#011val's rmse: 0.0571274\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[213]#011train's rmse: 0.060106#011val's rmse: 0.057051\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[214]#011train's rmse: 0.0600059#011val's rmse: 0.0569668\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[215]#011train's rmse: 0.0599329#011val's rmse: 0.0569137\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[216]#011train's rmse: 0.0598588#011val's rmse: 0.0568571\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[217]#011train's rmse: 0.0597865#011val's rmse: 0.0568224\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[218]#011train's rmse: 0.0597191#011val's rmse: 0.0567713\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[219]#011train's rmse: 0.059653#011val's rmse: 0.0567215\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[220]#011train's rmse: 0.0595869#011val's rmse: 0.0566712\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[221]#011train's rmse: 0.0595346#011val's rmse: 0.0566278\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[222]#011train's rmse: 0.0594884#011val's rmse: 0.0565831\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[223]#011train's rmse: 0.0594207#011val's rmse: 0.0565061\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[224]#011train's rmse: 0.0593753#011val's rmse: 0.0564596\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[225]#011train's rmse: 0.0593204#011val's rmse: 0.0564139\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[226]#011train's rmse: 0.0592622#011val's rmse: 0.056353\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[227]#011train's rmse: 0.0592052#011val's rmse: 0.0562853\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[228]#011train's rmse: 0.0591485#011val's rmse: 0.0562373\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[229]#011train's rmse: 0.0590934#011val's rmse: 0.0561715\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[230]#011train's rmse: 0.0590418#011val's rmse: 0.0561267\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[231]#011train's rmse: 0.058982#011val's rmse: 0.056082\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[232]#011train's rmse: 0.0589178#011val's rmse: 0.0560387\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[233]#011train's rmse: 0.058849#011val's rmse: 0.0560027\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[234]#011train's rmse: 0.0587868#011val's rmse: 0.0559611\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[235]#011train's rmse: 0.058726#011val's rmse: 0.0559205\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[236]#011train's rmse: 0.0586656#011val's rmse: 0.0558869\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[237]#011train's rmse: 0.0586058#011val's rmse: 0.055844\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[238]#011train's rmse: 0.0585468#011val's rmse: 0.0558086\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[239]#011train's rmse: 0.0584847#011val's rmse: 0.055773\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[240]#011train's rmse: 0.05842#011val's rmse: 0.0557372\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[241]#011train's rmse: 0.0583568#011val's rmse: 0.0557009\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[242]#011train's rmse: 0.0583021#011val's rmse: 0.0556765\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[243]#011train's rmse: 0.0582478#011val's rmse: 0.0556547\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[244]#011train's rmse: 0.0581851#011val's rmse: 0.0556205\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[245]#011train's rmse: 0.0581326#011val's rmse: 0.0556002\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[246]#011train's rmse: 0.0580819#011val's rmse: 0.0555621\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[247]#011train's rmse: 0.0580322#011val's rmse: 0.055525\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[248]#011train's rmse: 0.0579839#011val's rmse: 0.0554673\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[249]#011train's rmse: 0.0579365#011val's rmse: 0.0554107\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[250]#011train's rmse: 0.0578888#011val's rmse: 0.0553675\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[251]#011train's rmse: 0.0578438#011val's rmse: 0.0553293\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[252]#011train's rmse: 0.0577975#011val's rmse: 0.0553127\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[253]#011train's rmse: 0.057748#011val's rmse: 0.055296\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[254]#011train's rmse: 0.0577054#011val's rmse: 0.0552601\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[255]#011train's rmse: 0.0576519#011val's rmse: 0.0552313\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[256]#011train's rmse: 0.0575961#011val's rmse: 0.0551847\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[257]#011train's rmse: 0.05756#011val's rmse: 0.0551544\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[258]#011train's rmse: 0.0575142#011val's rmse: 0.0551433\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[259]#011train's rmse: 0.0574604#011val's rmse: 0.0550985\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[260]#011train's rmse: 0.0574113#011val's rmse: 0.055054\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[261]#011train's rmse: 0.0573712#011val's rmse: 0.0550166\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[262]#011train's rmse: 0.0573359#011val's rmse: 0.0549866\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[263]#011train's rmse: 0.0572964#011val's rmse: 0.0549548\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[264]#011train's rmse: 0.0572548#011val's rmse: 0.0549252\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[265]#011train's rmse: 0.0572146#011val's rmse: 0.0548969\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[266]#011train's rmse: 0.0571776#011val's rmse: 0.0548725\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[267]#011train's rmse: 0.0571286#011val's rmse: 0.0548153\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[268]#011train's rmse: 0.057093#011val's rmse: 0.0547921\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[269]#011train's rmse: 0.0570437#011val's rmse: 0.0547274\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[270]#011train's rmse: 0.0569943#011val's rmse: 0.0546692\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[271]#011train's rmse: 0.0569547#011val's rmse: 0.0546192\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[272]#011train's rmse: 0.0569063#011val's rmse: 0.0545769\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[273]#011train's rmse: 0.0568681#011val's rmse: 0.0545488\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[274]#011train's rmse: 0.0568344#011val's rmse: 0.0545009\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[275]#011train's rmse: 0.0567886#011val's rmse: 0.054462\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[276]#011train's rmse: 0.0567498#011val's rmse: 0.0544406\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[277]#011train's rmse: 0.0567111#011val's rmse: 0.0544363\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[278]#011train's rmse: 0.0566774#011val's rmse: 0.0544334\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[279]#011train's rmse: 0.05664#011val's rmse: 0.0544311\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[280]#011train's rmse: 0.056604#011val's rmse: 0.0543995\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[281]#011train's rmse: 0.0565535#011val's rmse: 0.0543622\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[282]#011train's rmse: 0.056514#011val's rmse: 0.0543327\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[283]#011train's rmse: 0.0564764#011val's rmse: 0.0543027\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[284]#011train's rmse: 0.0564397#011val's rmse: 0.0542734\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[285]#011train's rmse: 0.0564022#011val's rmse: 0.0542456\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[286]#011train's rmse: 0.0563666#011val's rmse: 0.0542282\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[287]#011train's rmse: 0.0563384#011val's rmse: 0.0542141\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[288]#011train's rmse: 0.0563085#011val's rmse: 0.0541857\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[289]#011train's rmse: 0.0562792#011val's rmse: 0.0541578\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[290]#011train's rmse: 0.0562553#011val's rmse: 0.0541429\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[291]#011train's rmse: 0.0562267#011val's rmse: 0.0541115\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[292]#011train's rmse: 0.0561986#011val's rmse: 0.0540806\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[293]#011train's rmse: 0.0561673#011val's rmse: 0.0540645\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[294]#011train's rmse: 0.0561377#011val's rmse: 0.0540465\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[295]#011train's rmse: 0.0561096#011val's rmse: 0.0540192\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[296]#011train's rmse: 0.0560877#011val's rmse: 0.0539878\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[297]#011train's rmse: 0.0560553#011val's rmse: 0.0539336\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[298]#011train's rmse: 0.0560432#011val's rmse: 0.0539228\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[299]#011train's rmse: 0.0560117#011val's rmse: 0.0538698\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[300]#011train's rmse: 0.0559907#011val's rmse: 0.0538487\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[301]#011train's rmse: 0.0559727#011val's rmse: 0.053821\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[302]#011train's rmse: 0.0559488#011val's rmse: 0.0537987\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[303]#011train's rmse: 0.0559276#011val's rmse: 0.0537793\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[304]#011train's rmse: 0.0558973#011val's rmse: 0.053763\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[305]#011train's rmse: 0.0558784#011val's rmse: 0.053741\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[306]#011train's rmse: 0.0558477#011val's rmse: 0.0537178\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[307]#011train's rmse: 0.0558162#011val's rmse: 0.0536946\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[308]#011train's rmse: 0.0557849#011val's rmse: 0.0536692\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[309]#011train's rmse: 0.055751#011val's rmse: 0.0536623\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[310]#011train's rmse: 0.0557178#011val's rmse: 0.0536559\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[311]#011train's rmse: 0.055688#011val's rmse: 0.0536846\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[312]#011train's rmse: 0.0556577#011val's rmse: 0.0536961\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[313]#011train's rmse: 0.0556292#011val's rmse: 0.0537121\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[314]#011train's rmse: 0.0556005#011val's rmse: 0.0537172\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[315]#011train's rmse: 0.0555726#011val's rmse: 0.0537365\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[316]#011train's rmse: 0.0555524#011val's rmse: 0.053717\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[317]#011train's rmse: 0.0555214#011val's rmse: 0.0537031\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[318]#011train's rmse: 0.0555004#011val's rmse: 0.0536987\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[319]#011train's rmse: 0.0554795#011val's rmse: 0.0536789\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[320]#011train's rmse: 0.0554585#011val's rmse: 0.0536757\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[321]#011train's rmse: 0.0554364#011val's rmse: 0.0536715\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[322]#011train's rmse: 0.0554005#011val's rmse: 0.053641\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[323]#011train's rmse: 0.0553664#011val's rmse: 0.0536146\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[324]#011train's rmse: 0.0553325#011val's rmse: 0.0535862\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[325]#011train's rmse: 0.0553095#011val's rmse: 0.053563\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[326]#011train's rmse: 0.055286#011val's rmse: 0.0535437\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[327]#011train's rmse: 0.0552656#011val's rmse: 0.0535131\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[328]#011train's rmse: 0.0552476#011val's rmse: 0.053493\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[329]#011train's rmse: 0.0552301#011val's rmse: 0.0534734\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[330]#011train's rmse: 0.0552116#011val's rmse: 0.0534539\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[331]#011train's rmse: 0.0551867#011val's rmse: 0.0534413\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[332]#011train's rmse: 0.0551634#011val's rmse: 0.0534354\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[333]#011train's rmse: 0.0551408#011val's rmse: 0.05343\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[334]#011train's rmse: 0.0551162#011val's rmse: 0.0534211\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[335]#011train's rmse: 0.0550974#011val's rmse: 0.0534249\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[336]#011train's rmse: 0.0550699#011val's rmse: 0.0534136\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[337]#011train's rmse: 0.055046#011val's rmse: 0.0534281\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[338]#011train's rmse: 0.0550263#011val's rmse: 0.0534269\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[339]#011train's rmse: 0.055006#011val's rmse: 0.0534366\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[340]#011train's rmse: 0.0549899#011val's rmse: 0.0534384\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[341]#011train's rmse: 0.0549579#011val's rmse: 0.0534147\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[342]#011train's rmse: 0.0549264#011val's rmse: 0.053386\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[343]#011train's rmse: 0.0548912#011val's rmse: 0.0533607\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[344]#011train's rmse: 0.0548623#011val's rmse: 0.0533327\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[345]#011train's rmse: 0.0548374#011val's rmse: 0.0533115\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[346]#011train's rmse: 0.0548171#011val's rmse: 0.05328\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[347]#011train's rmse: 0.0547943#011val's rmse: 0.0532578\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[348]#011train's rmse: 0.054778#011val's rmse: 0.0532316\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[349]#011train's rmse: 0.0547614#011val's rmse: 0.0532163\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[350]#011train's rmse: 0.0547457#011val's rmse: 0.0531908\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[351]#011train's rmse: 0.0547277#011val's rmse: 0.0531668\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[352]#011train's rmse: 0.0547079#011val's rmse: 0.053148\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[353]#011train's rmse: 0.0546883#011val's rmse: 0.0531146\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[354]#011train's rmse: 0.0546709#011val's rmse: 0.0530876\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[355]#011train's rmse: 0.0546581#011val's rmse: 0.0530717\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[356]#011train's rmse: 0.0546423#011val's rmse: 0.0530501\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[357]#011train's rmse: 0.0546266#011val's rmse: 0.053031\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[358]#011train's rmse: 0.0546145#011val's rmse: 0.0530029\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[359]#011train's rmse: 0.054603#011val's rmse: 0.0529665\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[360]#011train's rmse: 0.0545882#011val's rmse: 0.0529462\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[361]#011train's rmse: 0.0545569#011val's rmse: 0.052935\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[362]#011train's rmse: 0.0545264#011val's rmse: 0.0529224\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[363]#011train's rmse: 0.0544968#011val's rmse: 0.0529128\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[364]#011train's rmse: 0.0544733#011val's rmse: 0.0529097\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[365]#011train's rmse: 0.0544556#011val's rmse: 0.0528944\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[366]#011train's rmse: 0.054428#011val's rmse: 0.0528873\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[367]#011train's rmse: 0.0544055#011val's rmse: 0.0528773\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[368]#011train's rmse: 0.0543823#011val's rmse: 0.052882\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[369]#011train's rmse: 0.0543489#011val's rmse: 0.0528858\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[370]#011train's rmse: 0.0543229#011val's rmse: 0.05288\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[371]#011train's rmse: 0.054297#011val's rmse: 0.0528698\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[372]#011train's rmse: 0.0542713#011val's rmse: 0.052866\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[373]#011train's rmse: 0.0542452#011val's rmse: 0.0528776\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[374]#011train's rmse: 0.0542207#011val's rmse: 0.0528754\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[375]#011train's rmse: 0.0541995#011val's rmse: 0.0528784\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[376]#011train's rmse: 0.0541787#011val's rmse: 0.0528996\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[377]#011train's rmse: 0.0541545#011val's rmse: 0.0528972\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[378]#011train's rmse: 0.054133#011val's rmse: 0.0528959\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[379]#011train's rmse: 0.0541097#011val's rmse: 0.0528942\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[380]#011train's rmse: 0.0540786#011val's rmse: 0.0528843\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[381]#011train's rmse: 0.0540525#011val's rmse: 0.0528825\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[382]#011train's rmse: 0.0540347#011val's rmse: 0.0528804\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[383]#011train's rmse: 0.0540176#011val's rmse: 0.0528786\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[384]#011train's rmse: 0.0539963#011val's rmse: 0.0528466\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[385]#011train's rmse: 0.0539754#011val's rmse: 0.0528196\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[386]#011train's rmse: 0.0539576#011val's rmse: 0.052813\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[387]#011train's rmse: 0.0539372#011val's rmse: 0.0528137\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[388]#011train's rmse: 0.0539169#011val's rmse: 0.0528067\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[389]#011train's rmse: 0.0539027#011val's rmse: 0.052799\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[390]#011train's rmse: 0.053887#011val's rmse: 0.0527949\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[391]#011train's rmse: 0.0538676#011val's rmse: 0.0527866\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[392]#011train's rmse: 0.0538496#011val's rmse: 0.0527768\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[393]#011train's rmse: 0.0538326#011val's rmse: 0.0527654\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[394]#011train's rmse: 0.0538112#011val's rmse: 0.052754\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[395]#011train's rmse: 0.0537909#011val's rmse: 0.0527414\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[396]#011train's rmse: 0.0537715#011val's rmse: 0.0527152\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[397]#011train's rmse: 0.0537648#011val's rmse: 0.0526904\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[398]#011train's rmse: 0.0537583#011val's rmse: 0.0526687\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[399]#011train's rmse: 0.0537459#011val's rmse: 0.0526416\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[400]#011train's rmse: 0.0537314#011val's rmse: 0.05261\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[401]#011train's rmse: 0.0537125#011val's rmse: 0.0526054\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[402]#011train's rmse: 0.0536952#011val's rmse: 0.0525878\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[403]#011train's rmse: 0.0536847#011val's rmse: 0.0525865\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[404]#011train's rmse: 0.0536725#011val's rmse: 0.052587\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[405]#011train's rmse: 0.0536591#011val's rmse: 0.0525886\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[406]#011train's rmse: 0.0536487#011val's rmse: 0.0525746\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[407]#011train's rmse: 0.0536374#011val's rmse: 0.0525664\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[408]#011train's rmse: 0.0536267#011val's rmse: 0.0525752\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[409]#011train's rmse: 0.0536137#011val's rmse: 0.0525734\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[410]#011train's rmse: 0.0536036#011val's rmse: 0.052566\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[411]#011train's rmse: 0.053586#011val's rmse: 0.0525448\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[412]#011train's rmse: 0.0535696#011val's rmse: 0.0525247\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[413]#011train's rmse: 0.0535589#011val's rmse: 0.0525234\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[414]#011train's rmse: 0.0535441#011val's rmse: 0.0525049\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[415]#011train's rmse: 0.053528#011val's rmse: 0.0524851\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[416]#011train's rmse: 0.0535104#011val's rmse: 0.0524967\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[417]#011train's rmse: 0.0534867#011val's rmse: 0.0525285\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[418]#011train's rmse: 0.0534772#011val's rmse: 0.0525302\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[419]#011train's rmse: 0.0534643#011val's rmse: 0.0525312\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[420]#011train's rmse: 0.0534487#011val's rmse: 0.0525422\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[421]#011train's rmse: 0.0534339#011val's rmse: 0.0525509\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[422]#011train's rmse: 0.0534217#011val's rmse: 0.0525728\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[423]#011train's rmse: 0.0534074#011val's rmse: 0.0525827\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[424]#011train's rmse: 0.0533909#011val's rmse: 0.0526142\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[425]#011train's rmse: 0.0533777#011val's rmse: 0.0526329\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[426]#011train's rmse: 0.0533614#011val's rmse: 0.0526164\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[427]#011train's rmse: 0.0533471#011val's rmse: 0.0525996\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[428]#011train's rmse: 0.0533318#011val's rmse: 0.0525972\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[429]#011train's rmse: 0.053312#011val's rmse: 0.0525967\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[430]#011train's rmse: 0.0533064#011val's rmse: 0.0525946\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[431]#011train's rmse: 0.0532826#011val's rmse: 0.052564\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[432]#011train's rmse: 0.0532693#011val's rmse: 0.0525557\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[433]#011train's rmse: 0.0532576#011val's rmse: 0.0525577\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[434]#011train's rmse: 0.0532467#011val's rmse: 0.0525601\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[435]#011train's rmse: 0.0532307#011val's rmse: 0.0525576\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[436]#011train's rmse: 0.0532151#011val's rmse: 0.0525378\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[437]#011train's rmse: 0.0531989#011val's rmse: 0.0525319\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[438]#011train's rmse: 0.053183#011val's rmse: 0.0525227\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[439]#011train's rmse: 0.0531673#011val's rmse: 0.0525173\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[440]#011train's rmse: 0.053152#011val's rmse: 0.0525086\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[441]#011train's rmse: 0.0531368#011val's rmse: 0.0524994\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[442]#011train's rmse: 0.0531226#011val's rmse: 0.0524918\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[443]#011train's rmse: 0.0531026#011val's rmse: 0.0524857\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[444]#011train's rmse: 0.0530868#011val's rmse: 0.0524553\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[445]#011train's rmse: 0.0530713#011val's rmse: 0.0524256\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[446]#011train's rmse: 0.05306#011val's rmse: 0.0524312\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[447]#011train's rmse: 0.0530473#011val's rmse: 0.052446\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[448]#011train's rmse: 0.0530361#011val's rmse: 0.0524583\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[449]#011train's rmse: 0.0530156#011val's rmse: 0.0524707\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[450]#011train's rmse: 0.0529961#011val's rmse: 0.0524914\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[451]#011train's rmse: 0.0529687#011val's rmse: 0.052489\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[452]#011train's rmse: 0.0529395#011val's rmse: 0.0524611\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[453]#011train's rmse: 0.0529157#011val's rmse: 0.0524749\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[454]#011train's rmse: 0.0528972#011val's rmse: 0.0524637\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[455]#011train's rmse: 0.0528773#011val's rmse: 0.0524709\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[456]#011train's rmse: 0.0528658#011val's rmse: 0.0524689\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[457]#011train's rmse: 0.0528592#011val's rmse: 0.052479\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[458]#011train's rmse: 0.0528493#011val's rmse: 0.0524667\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[459]#011train's rmse: 0.052839#011val's rmse: 0.0524615\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[460]#011train's rmse: 0.0528296#011val's rmse: 0.0524498\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[461]#011train's rmse: 0.052822#011val's rmse: 0.0524531\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[462]#011train's rmse: 0.0528077#011val's rmse: 0.052465\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[463]#011train's rmse: 0.0527903#011val's rmse: 0.0524763\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[464]#011train's rmse: 0.0527794#011val's rmse: 0.0524813\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[465]#011train's rmse: 0.0527655#011val's rmse: 0.0524866\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[466]#011train's rmse: 0.0527537#011val's rmse: 0.0524934\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[467]#011train's rmse: 0.0527437#011val's rmse: 0.0524956\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[468]#011train's rmse: 0.0527366#011val's rmse: 0.0525021\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[469]#011train's rmse: 0.0527243#011val's rmse: 0.0524943\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[470]#011train's rmse: 0.0527144#011val's rmse: 0.0525028\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[471]#011train's rmse: 0.0526975#011val's rmse: 0.052515\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[472]#011train's rmse: 0.052681#011val's rmse: 0.0525274\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[473]#011train's rmse: 0.0526646#011val's rmse: 0.0525385\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[474]#011train's rmse: 0.0526473#011val's rmse: 0.0525326\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[475]#011train's rmse: 0.052632#011val's rmse: 0.0525453\u001b[0m\n",
      "\u001b[34mEarly stopping, best iteration is:\u001b[0m\n",
      "\u001b[34m[445]#011train's rmse: 0.0530713#011val's rmse: 0.0524256\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34mINFO:root:Info file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2023-04-03 22:27:53,836 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-04-03 22:28:38 Uploading - Uploading generated training model\n",
      "2023-04-03 22:28:38 Completed - Training job completed\n",
      "Training seconds: 101\n",
      "Billable seconds: 101\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Training job by passing the S3 path of the training data\n",
    "tabular_estimator.fit(\n",
    "    {\n",
    "        \"train\": training_dataset_s3_path,\n",
    "        \"validation\": validation_dataset_s3_path,\n",
    "    }, logs=True, job_name=training_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53cfc2",
   "metadata": {},
   "source": [
    "# Deploy and Run Inference on the Trained Tabular Model\n",
    "\n",
    "In this section, you learn how to query an existing endpoint and make predictions of the examples you input. For each example, the model will output a numerical value to estimate the corresponding target value.\n",
    "\n",
    "We start by retrieving the artifacts and deploy the tabular_estimator that we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3845c04-692d-4d20-8742-1ed029dd8587",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-04-03-22-32-06-802\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-lightgbm-regression-m-2023-04-03-22-32-06-802\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-lightgbm-regression-m-2023-04-03-22-32-06-802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.m5.large\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{train_model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "predictor = tabular_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51b79b",
   "metadata": {},
   "source": [
    "Next, we download a hold-out University test data from the S3 bucket for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6ac79",
   "metadata": {},
   "source": [
    "Next, we read the test data into pandas data frame, prepare the ground truth target and predicting features to send into the endpoint.\n",
    "\n",
    "Below is the first 5 examples in the test set. All of the test examples with features from Feature_1 to Feature_7 are sent into the deployed model to get model predictions, to estimate the ground truth Target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90ef6df-b0c0-4a37-aee3-c8f4d70f8a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecfdad63-17cc-4940-b202-1754e5d76ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the data from s3 buckets\n",
    "s3 = boto3.client(\"s3\")\n",
    "data_bucket = 'nr-jumpstart-9'\n",
    "\n",
    "data_path = \"data.csv\"\n",
    "\n",
    "# downloading the test data from data_bucket\n",
    "s3.download_file(data_bucket, 'test/data.csv', data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afc79fb1-0ade-40f9-9ead-3c0e5a9a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe test dataset contains 150 examples and 8 columns.\u001b[0m\n",
      "\n",
      "\u001b[1mThe first 5 observations of the test data: \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.61</td>\n",
       "      <td>300.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>321.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.83</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>305.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89</td>\n",
       "      <td>324.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.74</td>\n",
       "      <td>327.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    0.61      300.0       98.0        1.0        2.0        2.5       8.02   \n",
       "1    0.77      321.0      111.0        3.0        3.5        4.0       8.83   \n",
       "2    0.59      305.0      103.0        2.0        2.5        3.5       8.13   \n",
       "3    0.89      324.0      113.0        4.0        4.5        4.5       9.25   \n",
       "4    0.74      327.0      112.0        3.0        3.0        3.0       8.72   \n",
       "\n",
       "   Feature_7  \n",
       "0        0.0  \n",
       "1        1.0  \n",
       "2        0.0  \n",
       "3        1.0  \n",
       "4        1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline, bold, unbold = '\\n', '\\033[1m', '\\033[0m'\n",
    "\n",
    "# read the data\n",
    "test_data = pd.read_csv(data_path, header=None)\n",
    "test_data.columns = ['Target'] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the test data: {unbold}\")  # Feature_1 is the categorical variables and rest of other features are numeric variables.\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7af5a",
   "metadata": {},
   "source": [
    "The following code queries the endpoint you have created to get the prediction for each test example. The query_endpoint() function returns a array-like of shape (num_examples, )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "640daecd-d44c-44c0-9052-17a0e49bba45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_type = \"text/csv\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_resonse(query_response):\n",
    "    predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return np.array(predictions[\"prediction\"])\n",
    "\n",
    "\n",
    "query_response = query_endpoint(features.to_csv(header=False, index=False).encode(\"utf-8\"))\n",
    "model_predictions = parse_resonse(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb481",
   "metadata": {},
   "source": [
    "# Evaluate the Prediction Results Returned from the Endpoint\n",
    "\n",
    "We evaluate the predictions results returned from the endpoint by following two ways.\n",
    "\n",
    "Visualize the prediction results by a residual plot to compare the model predictions and ground truth targets.\n",
    "\n",
    "Measure the prediction results quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f40624f-c849-48b9-ba66-4d9d9b14516b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization: a residual plot to compare the model predictions and ground truth targets. For each example, the residual value\n",
    "# is the subtraction between the prediction and ground truth target.\n",
    "# We can see that the points in the residual plot are randomly dispersed around the horizontal axis y = 0,\n",
    "# which indicates the fitted regression model is appropriate for the ABALONE data\n",
    "\n",
    "residuals = ground_truth_label.values[:, 0] - model_predictions\n",
    "plt.scatter(model_predictions, residuals, color=\"blue\", s=40)\n",
    "plt.hlines(y=0, xmin=4, xmax=18)\n",
    "plt.xlabel(\"Predicted Values\", fontsize=18)\n",
    "plt.ylabel(\"Residuals\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a621047f-354d-4762-8fb9-aee12736f863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEvaluation result on test data\u001b[0m:\n",
      "\u001b[1mr2_score\u001b[0m: 0.8347803463244985\n",
      "\u001b[1mmean_squared_error\u001b[0m: 0.0032756969897500972\n",
      "\u001b[1mmean_absolute_error\u001b[0m: 0.041440139984582014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model predictions quantitatively.\n",
    "eval_r2_score = r2_score(ground_truth_label.values, model_predictions)\n",
    "eval_mse_score = mean_squared_error(ground_truth_label.values, model_predictions)\n",
    "eval_mae_score = mean_absolute_error(ground_truth_label.values, model_predictions)\n",
    "print(\n",
    "    f\"{bold}Evaluation result on test data{unbold}:{newline}\"\n",
    "    f\"{bold}{r2_score.__name__}{unbold}: {eval_r2_score}{newline}\"\n",
    "    f\"{bold}{mean_squared_error.__name__}{unbold}: {eval_mse_score}{newline}\"\n",
    "    f\"{bold}{mean_absolute_error.__name__}{unbold}: {eval_mae_score}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8a973-f920-4fb1-bbca-5defb65450b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
